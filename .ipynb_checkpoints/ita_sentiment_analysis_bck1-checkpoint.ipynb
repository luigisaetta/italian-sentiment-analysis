{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0ba0db",
   "metadata": {},
   "source": [
    "### Sentiment Analysis in Italian using Transformers\n",
    "\n",
    "based on **Neuraly** work; See:\n",
    "* see: https://huggingface.co/neuraly/bert-base-italian-cased-sentiment\n",
    "* https://medium.com/@a.bellini/leveraging-huggingfaces-transformers-for-cross-lingual-sentiment-analysis-acca1f4e9da6\n",
    "\n",
    "for more details, see also:\n",
    "* https://huggingface.co/blog/sentiment-analysis-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344f1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# HuggingFace transformers (availale in OCI DS conda nlp env)\n",
    "# see: https://github.com/huggingface/transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10222ced",
   "metadata": {},
   "source": [
    "### Python class\n",
    "\n",
    "I have encapsulated the code from the HF site of the model, with some semplification, in a Python class.\n",
    "\n",
    "* Conda env used: Natural Language Processing for CPU Python 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fde579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITASentimentAnalyzer:\n",
    "    # load the tokenizer and transformer\n",
    "    def __init__(self, MODEL_NAME):\n",
    "\n",
    "        #\n",
    "        # attribute definitions\n",
    "        #\n",
    "\n",
    "        # for rounding the scores\n",
    "        self._DEC_DIGITS = 4\n",
    "\n",
    "        # name of HuggingFace model used\n",
    "        self._MODEL_NAME = MODEL_NAME\n",
    "\n",
    "        # the list of defined labels (and therefore we will have 3 scores)\n",
    "        self._LABELS = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "        print(\"Loading model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self._MODEL_NAME)\n",
    "        # Load the model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self._MODEL_NAME\n",
    "        )\n",
    "\n",
    "        print(\"Model loading completed!\")\n",
    "\n",
    "    #\n",
    "    # does the scoring on a single sentence a time\n",
    "    #\n",
    "    def score(self, input_sentence):\n",
    "        # encode the sentence and create the input tensor (in PyTorch format)\n",
    "        input_ids = self.tokenizer(\n",
    "            input_sentence, add_special_tokens=True, return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # output from tokenizer is already a tensor\n",
    "\n",
    "        # Call the model and get the logits\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids)[\"logits\"]\n",
    "\n",
    "        # The model was trained with a Log Likelyhood + Softmax combined loss, hence to extract probabilities we need a softmax on top of the logits tensor\n",
    "        proba = nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "        # to remove the added dimension with squeeze\n",
    "        # proba is (negative, neutral, positive)\n",
    "        scores = proba.squeeze(0)\n",
    "\n",
    "        # get rid of tensor and round and\n",
    "        # prepare the output json\n",
    "\n",
    "        ret_vet = []\n",
    "\n",
    "        for i, label in enumerate(self._LABELS):\n",
    "            ret_vet.append({\"label\": label, \"score\": self.round(scores[i])})\n",
    "\n",
    "        return ret_vet\n",
    "\n",
    "    #\n",
    "    # works on a list of sentences, with a single call to model\n",
    "    #\n",
    "    def batch_score(self, input_sentences):\n",
    "        # encode the sentence and create the input tensor (in PyTorch format)\n",
    "        # requires padding\n",
    "        tokens = self.tokenizer(\n",
    "            input_sentences, add_special_tokens=True, padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # output from tokenizer is already a tensor\n",
    "\n",
    "        # Call the model and get the logits\n",
    "        # in batch scoring I need also to pass the attention mask\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**tokens)[\"logits\"]\n",
    "\n",
    "        proba = nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "        return proba\n",
    "\n",
    "    # only to format output\n",
    "    def format_scores(self, scores):\n",
    "        score_str = \"\"\n",
    "\n",
    "        for v_score in scores:\n",
    "            score_str += str(v_score[\"label\"]) + \": \" + str(v_score[\"score\"]) + \"|\"\n",
    "\n",
    "        return score_str\n",
    "\n",
    "    # utility to get rid of tensor and round\n",
    "    def round(self, tens_val):\n",
    "        return round(tens_val.item(), self._DEC_DIGITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983768c",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15ff0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loading completed!\n",
      "CPU times: user 861 ms, sys: 156 ms, total: 1.02 s\n",
      "Wall time: 5.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# loading the model: pass the HF model name\n",
    "MODEL_NAME = \"neuraly/bert-base-italian-cased-sentiment\"\n",
    "\n",
    "sent_analyzer = ITASentimentAnalyzer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfc1e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 177 ms, sys: 6.82 ms, total: 184 ms\n",
      "Wall time: 47.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9716},\n",
       " {'label': 'neutral', 'score': 0.0181},\n",
       " {'label': 'positive', 'score': 0.0103}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scores = sent_analyzer.score(\n",
    "    \"Non credo che la sua organizzazione abbia fornito un buon servizio alla clientela\"\n",
    ")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133247a5",
   "metadata": {},
   "source": [
    "### test on a set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc7b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. E' un prodotto pessimo\n",
      "negative: 0.9978|neutral: 0.002|positive: 0.0002|\n",
      "\n",
      "2. La sua organizzazione ha fornito un buon servizio alla clientela\n",
      "negative: 0.0002|neutral: 0.002|positive: 0.9978|\n",
      "\n",
      "3. La sua organizzazione non ha fornito un buon servizio alla clientela\n",
      "negative: 0.9951|neutral: 0.0046|positive: 0.0003|\n",
      "\n",
      "4. Non credo che la sua organizzazione abbia fornito un buon servizio alla clientela\n",
      "negative: 0.9716|neutral: 0.0181|positive: 0.0103|\n",
      "\n",
      "5. Il prodotto non funziona, non comprero' più nulla dalla vostra azienda\n",
      "negative: 0.9976|neutral: 0.0023|positive: 0.0002|\n",
      "\n",
      "6. Io penso che la sua organizzazione non abbia fornito un buon servizio alla clientela\n",
      "negative: 0.9973|neutral: 0.0025|positive: 0.0002|\n",
      "\n",
      "7. La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona?\n",
      "negative: 0.0006|neutral: 0.8939|positive: 0.1055|\n",
      "\n",
      "8. La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona\n",
      "negative: 0.0004|neutral: 0.0487|positive: 0.9509|\n",
      "\n",
      "9. La vostra organizzazione offre servizi pessimi\n",
      "negative: 0.9852|neutral: 0.0146|positive: 0.0002|\n",
      "\n",
      "10. La vostra organizzazione offre servizi non adeguati\n",
      "negative: 0.9593|neutral: 0.0405|positive: 0.0002|\n",
      "\n",
      "11. Sono molto soddisfatto del tuo lavoro\n",
      "negative: 0.0002|neutral: 0.0015|positive: 0.9984|\n",
      "\n",
      "12. non sono del tutto sicuro che il lavoro sia adeguato\n",
      "negative: 0.6756|neutral: 0.3186|positive: 0.0058|\n",
      "\n",
      "13. l'azienda dovrebbe offrire servizi migliori\n",
      "negative: 0.0035|neutral: 0.0057|positive: 0.9908|\n",
      "\n",
      "14. il supporto offerto dal customer care non è stato adeguato\n",
      "negative: 0.9884|neutral: 0.0114|positive: 0.0002|\n",
      "\n",
      "15. il risultato è pessimo\n",
      "negative: 0.9974|neutral: 0.0025|positive: 0.0002|\n",
      "\n",
      "16. il Napoli ha giocato una partita decente\n",
      "negative: 0.1219|neutral: 0.7695|positive: 0.1086|\n",
      "\n",
      "17. il lavoro dell'allenatore è stato modesto\n",
      "negative: 0.0141|neutral: 0.9705|positive: 0.0154|\n",
      "\n",
      "\n",
      "CPU times: user 3.4 s, sys: 47.9 ms, total: 3.45 s\n",
      "Wall time: 862 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_sentences = [\n",
    "    \"E' un prodotto pessimo\",\n",
    "    \"La sua organizzazione ha fornito un buon servizio alla clientela\",\n",
    "    \"La sua organizzazione non ha fornito un buon servizio alla clientela\",\n",
    "    \"Non credo che la sua organizzazione abbia fornito un buon servizio alla clientela\",\n",
    "    \"Il prodotto non funziona, non comprero' più nulla dalla vostra azienda\",\n",
    "    \"Io penso che la sua organizzazione non abbia fornito un buon servizio alla clientela\",\n",
    "    \"La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona?\",\n",
    "    \"La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona\",\n",
    "    \"La vostra organizzazione offre servizi pessimi\",\n",
    "    \"La vostra organizzazione offre servizi non adeguati\",\n",
    "    \"Sono molto soddisfatto del tuo lavoro\",\n",
    "    \"non sono del tutto sicuro che il lavoro sia adeguato\",\n",
    "    \"l'azienda dovrebbe offrire servizi migliori\",\n",
    "    \"il supporto offerto dal customer care non è stato adeguato\",\n",
    "    \"il risultato è pessimo\",\n",
    "    \"il Napoli ha giocato una partita decente\",\n",
    "    \"il lavoro dell'allenatore è stato modesto\",\n",
    "]\n",
    "\n",
    "\n",
    "# object already instantiated\n",
    "\n",
    "for i, sentence in enumerate(input_sentences):\n",
    "\n",
    "    #\n",
    "    # here I do the scoring on a single sentence\n",
    "    #\n",
    "    scores = sent_analyzer.score(sentence)\n",
    "\n",
    "    # scores is a dict of []\n",
    "\n",
    "    print(f\"{i+1}. {sentence}\")\n",
    "    print(sent_analyzer.format_scores(scores))\n",
    "    print()\n",
    "\n",
    "# formatting\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f4388a",
   "metadata": {},
   "source": [
    "### next step: Test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc889e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 35.6 ms, total: 1.44 s\n",
      "Wall time: 361 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.97822404e-01, 1.98977720e-03, 1.87821424e-04],\n",
       "       [2.41547939e-04, 1.96292158e-03, 9.97795582e-01],\n",
       "       [9.95064676e-01, 4.62475792e-03, 3.10573465e-04],\n",
       "       [9.71631527e-01, 1.80967916e-02, 1.02717374e-02],\n",
       "       [9.97555971e-01, 2.27070157e-03, 1.73314256e-04],\n",
       "       [9.97273028e-01, 2.51539331e-03, 2.11581835e-04],\n",
       "       [6.31291943e-04, 8.93854976e-01, 1.05513722e-01],\n",
       "       [4.11611923e-04, 4.86868806e-02, 9.50901508e-01],\n",
       "       [9.85226452e-01, 1.46010835e-02, 1.72518616e-04],\n",
       "       [9.59285676e-01, 4.05113772e-02, 2.02959549e-04],\n",
       "       [1.60809665e-04, 1.45011442e-03, 9.98389125e-01],\n",
       "       [6.75603926e-01, 3.18588316e-01, 5.80779370e-03],\n",
       "       [3.46464734e-03, 5.73630678e-03, 9.90799069e-01],\n",
       "       [9.88429904e-01, 1.14101814e-02, 1.59893607e-04],\n",
       "       [9.97361124e-01, 2.45943945e-03, 1.79441457e-04],\n",
       "       [1.21894449e-01, 7.69497573e-01, 1.08607978e-01],\n",
       "       [1.41019728e-02, 9.70547378e-01, 1.53505998e-02]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scores = sent_analyzer.batch_score(input_sentences)\n",
    "\n",
    "# Instead of a tensor I want the numpy vector\n",
    "scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f472cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_p37_cpu_v2]",
   "language": "python",
   "name": "conda-env-nlp_p37_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
