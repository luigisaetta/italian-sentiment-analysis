{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ff2543",
   "metadata": {},
   "source": [
    "### Sentiment Analysis in Italian. Model1\n",
    "\n",
    "* see: https://huggingface.co/neuraly/bert-base-italian-cased-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46bdd79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# to save in Model Catalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from ads import set_auth\n",
    "\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from ads.common.model_metadata import (MetadataCustomCategory,\n",
    "                                       UseCaseType,\n",
    "                                       Framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe41d9e",
   "metadata": {},
   "source": [
    "### The first two functions download the pretrained model from Internet.\n",
    "Then the model is saved in model-files directory and saved to the model catalog. This way in score.py we don't need download from Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca70a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "tokenizer = None\n",
    "# Load the model\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309cbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_load_model():\n",
    "    global tokenizer, model\n",
    "    \n",
    "    # Load the tokenizer and the model\n",
    "    MODEL_NAME = \"neuraly/bert-base-italian-cased-sentiment\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce02a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(input_sentence):\n",
    "    # encode the sentence and create the input tensor\n",
    "    input_ids = tokenizer.encode(input_sentence, add_special_tokens=True)\n",
    "\n",
    "    # Create tensor for input\n",
    "    tensor = torch.tensor(input_ids).long()\n",
    "    \n",
    "    # add the baych dimension (not needed if we're scoring on N sentences)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Call the model and get the logits\n",
    "    logits = model(tensor)\n",
    "\n",
    "    # Remove the fake batch dimension\n",
    "    # I changed from the url this line of code to avoid an exception... This way it works\n",
    "    logits = logits['logits'].squeeze(0)\n",
    "\n",
    "    # The model was trained with a Log Likelyhood + Softmax combined loss, hence to extract probabilities we need a softmax on top of the logits tensor\n",
    "    proba = nn.functional.softmax(logits, dim=0)\n",
    "    \n",
    "    # proba is (negative, neutral, positive)\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c5e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and tokenizer\n",
    "my_load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ada2d",
   "metadata": {},
   "source": [
    "### scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b8b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 556 ms, sys: 10.2 ms, total: 566 ms\n",
      "Wall time: 142 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is the sentence we're using for our tests\n",
    "# sentence = \"Beh, l'azienda XXXX dovrebbe provare ad offrire servizi migliori, i servizi attuali non sono adeguati e costano tanto\"\n",
    "sentence = \"La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata ottima\"\n",
    "\n",
    "negative, neutral, positive = my_predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b317f545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative score: 0.0001\n",
      "Neutral score: 0.0058\n",
      "Positive score: 0.994\n"
     ]
    }
   ],
   "source": [
    "print('Negative score:', round(negative.item(), 4))\n",
    "print('Neutral score:', round(neutral.item(), 4))\n",
    "print('Positive score:', round(positive.item(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa7d17",
   "metadata": {},
   "source": [
    "### Save in the model catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7930aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_auth(auth='resource_principal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e712af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loop1', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ADS:As force_overwrite is set to True, all the existing files in the /home/datascience/model-files will be removed\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:ads.common.model_artifact:We give you the option to specify a different inference conda environment for model deployment purposes. By default it is assumed to be the same as the conda environment used to train the model. If you wish to specify a different environment for inference purposes, please assign the path of a published or data science conda environment to the optional parameter `inference_conda_env`. \n",
      "WARNING:ADS:Taxonomy metadata was not extracted. To auto-populate taxonomy metadata the model must be provided. Pass the model as a parameter to .prepare_generic_model(model=model, usecase_type=UseCaseType.REGRESSION). Alternative way is using atifact.populate_metadata(model=model, usecase_type=UseCaseType.REGRESSION).\n",
      "INFO:ads.common.model_artifact:To auto-extract taxonomy metadata the model must be provided. Supported models: automl, keras, lightgbm, pytorch, sklearn, tensorflow, and xgboost.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/datascience/model-files/tokenizer_config.json',\n",
       " '/home/datascience/model-files/special_tokens_map.json',\n",
       " '/home/datascience/model-files/vocab.txt',\n",
       " '/home/datascience/model-files/added_tokens.json',\n",
       " '/home/datascience/model-files/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. prepare artifacts directory\n",
    "\n",
    "PATH_ARTEFACT = \"/home/datascience/model-files\"\n",
    "\n",
    "artifact = prepare_generic_model(PATH_ARTEFACT, force_overwrite=True, data_science_env=True, \n",
    "                                 use_case_type=UseCaseType.SENTIMENT_ANALYSIS)\n",
    "\n",
    "# model and tokenizer are saved\n",
    "model.save_pretrained(PATH_ARTEFACT)\n",
    "tokenizer.save_pretrained(PATH_ARTEFACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ed1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/model-files/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PATH_ARTEFACT}/score.py\n",
    "\n",
    "#\n",
    "# customize and save score.py\n",
    "#\n",
    "import torch\n",
    "from torch import nn  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import logging \n",
    "\n",
    "# logging configuration - OPTIONAL \n",
    "logging.basicConfig(format='%(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger_pred = logging.getLogger('model-prediction')\n",
    "logger_pred.setLevel(logging.INFO)\n",
    "logger_feat = logging.getLogger('input-features')\n",
    "logger_feat.setLevel(logging.INFO)\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "# to enable/disable detailed logging\n",
    "DEBUG = True\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    global tokenizer, model\n",
    "    \n",
    "    logger_pred.info(\"Entering in load model..\")\n",
    "    \n",
    "    tokenizer_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    logger_pred.info(tokenizer_dir)\n",
    "    \n",
    "    # Load the tokenizer and the model\n",
    "    # MODEL_NAME = \"neuraly/bert-base-italian-cased-sentiment\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    logger_pred.info(\"loaded tokenizer\")\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(tokenizer_dir)\n",
    "    \n",
    "    logger_pred.info(\"Loaded model...\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict(data, model=load_model()) -> dict:\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: { 'prediction': output from `model.predict` method }\n",
    "\n",
    "    \"\"\"\n",
    "    global tokenizer\n",
    "    \n",
    "    # model contains the model and the scaler\n",
    "    logger_pred.info(\"In predict...\")\n",
    "    \n",
    "    # some check\n",
    "    assert model is not None, \"Model is not loaded\"\n",
    "    \n",
    "    logger_pred.info(\"Invoking model......\")\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    # processing one row a time\n",
    "    for sentence in data:\n",
    "        logger_pred.info('Processing: ' + sentence)\n",
    "        \n",
    "        # encode the sentence and create the input tensor\n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "\n",
    "        # Create tensor for input\n",
    "        tensor = torch.tensor(input_ids).long()\n",
    "    \n",
    "        # add the baych dimension (not needed if we're scoring on N sentences)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "        # Call the model and get the logits\n",
    "        logits = model(tensor)\n",
    "\n",
    "        # Remove the fake batch dimension\n",
    "        # I changed from the url this line of code to avoid an exception... This way it works\n",
    "        logits = logits['logits'].squeeze(0)\n",
    "\n",
    "        # The model was trained with a Log Likelyhood + Softmax combined loss, hence to extract probabilities we need a softmax on top of the logits tensor\n",
    "        proba = nn.functional.softmax(logits, dim=0)\n",
    "        \n",
    "        outputs.append(proba.detach().numpy())\n",
    "    \n",
    "        # proba is (negative, neutral, positive)\n",
    "    \n",
    "    return { 'prediction': outputs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7572b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:ads.common.model_artifact:{\n",
      "  \"git_branch\": \"main\",\n",
      "  \"git_commit\": \"a3c6a86c1fd7229f501702aa2fcaea5510f70a51\",\n",
      "  \"repository_url\": \"https://github.com/luigisaetta/italian-sentiment-analysis.git\",\n",
      "  \"script_dir\": \"/home/datascience/model-files\",\n",
      "  \"training_id\": null,\n",
      "  \"training_script\": \"None\"\n",
      "}\n",
      "['runtime.yaml', 'vocab.txt', 'config.json', 'special_tokens_map.json', 'score.py', 'tokenizer_config.json', 'pytorch_model.bin', 'tokenizer.json']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loop1', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact:/tmp/saved_model_2b47ceec-adb0-46b9-9d3e-74b9991a1f6b.zip\n"
     ]
    }
   ],
   "source": [
    "catalog_entry = artifact.save(display_name='model-sentiment1', description='A model for Sentiment Analysis using Tranformers', timeout=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65129f6",
   "metadata": {},
   "source": [
    "### Test the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "553e3e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:model-prediction:/home/datascience/model-files\n",
      "INFO:model-prediction:loaded tokenizer\n",
      "INFO:model-prediction:Loaded model...\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import sys \n",
    "sys.path.insert(0, PATH_ARTEFACT)\n",
    "\n",
    "import score\n",
    "\n",
    "from score import load_model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd1b4107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:model-prediction:/home/datascience/model-files\n",
      "INFO:model-prediction:loaded tokenizer\n",
      "INFO:model-prediction:Loaded model...\n"
     ]
    }
   ],
   "source": [
    "model = score.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dde83d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:model-prediction:In predict...\n",
      "INFO:model-prediction:Invoking model......\n",
      "INFO:model-prediction:Processing: La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata ottima\n",
      "INFO:model-prediction:Processing: La vostra azienda offre servizi pessimi\n",
      "INFO:model-prediction:Processing: Sono molto soddisfatto del tuo lavoro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': [array([1.3733095e-04, 5.8355918e-03, 9.9402702e-01], dtype=float32),\n",
       "  array([9.8522472e-01, 1.4600362e-02, 1.7503665e-04], dtype=float32),\n",
       "  array([1.6080952e-04, 1.4501130e-03, 9.9838912e-01], dtype=float32)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.predict([\"La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata ottima\",\n",
    "              \"La vostra azienda offre servizi pessimi\",\n",
    "              \"Sono molto soddisfatto del tuo lavoro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f9df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_p37_cpu_v2]",
   "language": "python",
   "name": "conda-env-nlp_p37_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
