{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42283199",
   "metadata": {},
   "source": [
    "### Sentiment Analysis in Italian. Model1\n",
    "\n",
    "* see: https://huggingface.co/neuraly/bert-base-italian-cased-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e50777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# to save in Model Catalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from ads import set_auth\n",
    "\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from ads.common.model_metadata import (MetadataCustomCategory,\n",
    "                                       UseCaseType,\n",
    "                                       Framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c2f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "tokenizer = None\n",
    "# Load the model\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4efe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_load_model():\n",
    "    global tokenizer, model\n",
    "    \n",
    "    # Load the tokenizer and the model\n",
    "    MODEL_NAME = \"neuraly/bert-base-italian-cased-sentiment\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab30ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(input_sentence):\n",
    "    # encode the sentence and create the input tensor\n",
    "    input_ids = tokenizer.encode(input_sentence, add_special_tokens=True)\n",
    "\n",
    "    # Create tensor for input\n",
    "    tensor = torch.tensor(input_ids).long()\n",
    "    \n",
    "    # add the baych dimension (not needed if we're scoring on N sentences)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Call the model and get the logits\n",
    "    logits = model(tensor)\n",
    "\n",
    "    # Remove the fake batch dimension\n",
    "    # I changed from the url this line of code to avoid an exception... This way it works\n",
    "    logits = logits['logits'].squeeze(0)\n",
    "\n",
    "    # The model was trained with a Log Likelyhood + Softmax combined loss, hence to extract probabilities we need a softmax on top of the logits tensor\n",
    "    proba = nn.functional.softmax(logits, dim=0)\n",
    "    \n",
    "    # proba is (negative, neutral, positive)\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e8f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and tokenizer\n",
    "my_load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b2381",
   "metadata": {},
   "source": [
    "### scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df6b596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 566 ms, sys: 11.5 ms, total: 577 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is the sentence we're using for our tests\n",
    "# sentence = \"Beh, l'azienda XXXX dovrebbe provare ad offrire servizi migliori, i servizi attuali non sono adeguati e costano tanto\"\n",
    "sentence = \"La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata adeguata\"\n",
    "\n",
    "negative, neutral, positive = my_predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702c63b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative score: 0.0014\n",
      "Neutral score: 0.9472\n",
      "Positive score: 0.0514\n"
     ]
    }
   ],
   "source": [
    "print('Negative score:', round(negative.item(), 4))\n",
    "print('Neutral score:', round(neutral.item(), 4))\n",
    "print('Positive score:', round(positive.item(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169768c",
   "metadata": {},
   "source": [
    "### Save in the model catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f97b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_auth(auth='resource_principal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad277ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loop1', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ADS:As force_overwrite is set to True, all the existing files in the /home/datascience/model-files will be removed\n",
      "INFO:ads.common.model_artifact:We give you the option to specify a different inference conda environment for model deployment purposes. By default it is assumed to be the same as the conda environment used to train the model. If you wish to specify a different environment for inference purposes, please assign the path of a published or data science conda environment to the optional parameter `inference_conda_env`. \n",
      "WARNING:ads.common:Auto-extraction of taxonomy is not supported for the provided model. The supported models are lightgbm, xgboost, sklearn, keras, tensorflow, bert, transformers, torch, automl.\n"
     ]
    }
   ],
   "source": [
    "# 1. prepare artifacts directory\n",
    "\n",
    "PATH_ARTEFACT = \"/home/datascience/model-files\"\n",
    "\n",
    "artifact = prepare_generic_model(PATH_ARTEFACT, model=model, force_overwrite=True, data_science_env=True, \n",
    "                                 use_case_type=UseCaseType.SENTIMENT_ANALYSIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985309ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/model-files/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PATH_ARTEFACT}/score.py\n",
    "\n",
    "#\n",
    "# customize and save score.py\n",
    "#\n",
    "import torch\n",
    "from torch import nn  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import logging \n",
    "\n",
    "# logging configuration - OPTIONAL \n",
    "logging.basicConfig(format='%(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger_pred = logging.getLogger('model-prediction')\n",
    "logger_pred.setLevel(logging.INFO)\n",
    "logger_feat = logging.getLogger('input-features')\n",
    "logger_feat.setLevel(logging.INFO)\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "# to enable/disable detailed logging\n",
    "DEBUG = True\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    global tokenizer, model\n",
    "    \n",
    "    # Load the tokenizer and the model\n",
    "    MODEL_NAME = \"neuraly/bert-base-italian-cased-sentiment\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    print(\"Loaded model...\")\n",
    "    logger_pred.info(\"Loaded model...\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict(data, model=load_model()) -> dict:\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: { 'prediction': output from `model.predict` method }\n",
    "\n",
    "    \"\"\"\n",
    "    # model contains the model and the scaler\n",
    "    logger_pred.info(\"In predict...\")\n",
    "    \n",
    "    # some check\n",
    "    assert model is not None, \"Model is not loaded\"\n",
    "    \n",
    "    logger_pred.info(\"Invoking model......\")\n",
    "    \n",
    "    # encode the sentence and create the input tensor\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "\n",
    "    # Create tensor for input\n",
    "    tensor = torch.tensor(input_ids).long()\n",
    "    \n",
    "    # add the baych dimension (not needed if we're scoring on N sentences)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Call the model and get the logits\n",
    "    logits = model(tensor)\n",
    "\n",
    "    # Remove the fake batch dimension\n",
    "    # I changed from the url this line of code to avoid an exception... This way it works\n",
    "    logits = logits['logits'].squeeze(0)\n",
    "\n",
    "    # The model was trained with a Log Likelyhood + Softmax combined loss, hence to extract probabilities we need a softmax on top of the logits tensor\n",
    "    proba = nn.functional.softmax(logits, dim=0)\n",
    "    \n",
    "    # proba is (negative, neutral, positive)\n",
    "    \n",
    "    return { 'prediction': proba }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "819febe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ads.common.model_artifact:{\n",
      "  \"git_branch\": \"None\",\n",
      "  \"git_commit\": \"None\",\n",
      "  \"repository_url\": \"None\",\n",
      "  \"script_dir\": \"/home/datascience/model-files\",\n",
      "  \"training_id\": null,\n",
      "  \"training_script\": \"None\"\n",
      "}\n",
      "['runtime.yaml', 'score.py']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loop1', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact:/tmp/saved_model_b40f6664-92df-4528-9e6a-f0382dcffbf3.zip\n"
     ]
    }
   ],
   "source": [
    "catalog_entry = artifact.save(display_name='model-sentiment1', description='A model for Sentiment Analysis using Tranformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5deb3c",
   "metadata": {},
   "source": [
    "### Test the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, PATH_ARTEFACT)\n",
    "\n",
    "import score\n",
    "\n",
    "from score import load_model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab748ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = score.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score.predict(\"E' un giocatore finito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f551a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_p37_cpu_v2]",
   "language": "python",
   "name": "conda-env-nlp_p37_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
