{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ade1dca",
   "metadata": {},
   "source": [
    "### Sentiment Analysis in Italian. Model1\n",
    "\n",
    "* see: https://huggingface.co/neuraly/bert-base-italian-cased-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc22f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228077fa",
   "metadata": {},
   "source": [
    "### The first two functions download the pretrained model from Internet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6231071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "THR = 0.3\n",
    "DEC_DIGITS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81305a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Load the tokenizer and the model. The first one is better\n",
    "    MODEL_NAME = \"neuraly/bert-base-italian-cased-sentiment\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50444499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sentence, tokenizer, model):\n",
    "    # encode the sentence and create the input tensor\n",
    "    input_ids = tokenizer(input_sentence, add_special_tokens=True, padding=True)['input_ids']\n",
    "\n",
    "    # Create tensor for input\n",
    "    tensor = torch.tensor(input_ids).long()\n",
    "    \n",
    "    # add the batch dimension (not needed if we're scoring on N sentences)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Call the model and get the logits\n",
    "    logits = model(tensor)['logits']\n",
    "\n",
    "    # Remove the fake batch dimension\n",
    "    # I changed from the url this line of code to avoid an exception... This way it works\n",
    "    # logits = logits.squeeze(0)\n",
    "\n",
    "    # The model was trained with a Log Likelyhood + Softmax combined loss, hence to extract probabilities we need a softmax on top of the logits tensor\n",
    "    proba = nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    # after the softmax to use same dim=1\n",
    "    proba = proba.squeeze(0)\n",
    "    \n",
    "    # proba is (negative, neutral, positive)\n",
    "    # [0] to remove the added dimension\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "696c797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this one has as input a list of sentences\n",
    "#\n",
    "def batch_predict(input_sentences, tokenizer, model):\n",
    "    # encode the sentence and create the input tensor\n",
    "    input_ids = tokenizer(input_sentences, add_special_tokens=True, padding=True)['input_ids']\n",
    "    \n",
    "    # Create tensor for input\n",
    "    tensor = torch.tensor(input_ids).long()\n",
    "\n",
    "    # Call the model and get the logits\n",
    "    logits = model(tensor)\n",
    "    \n",
    "    print(logits)\n",
    "    \n",
    "    logits = logits['logits']\n",
    "\n",
    "    # Remove the fake batch dimension\n",
    "    # I changed from the url this line of code to avoid an exception... This way it works\n",
    "\n",
    "    # The model was trained with a Log Likelyhood + Softmax combined loss, hence to extract probabilities we need a softmax on top of the logits tensor\n",
    "    print(logits.size())\n",
    "    proba = nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    # proba is (negative, neutral, positive)\n",
    "\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16efec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 836 ms, sys: 118 ms, total: 954 ms\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# loading model and tokenizer\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ba33e",
   "metadata": {},
   "source": [
    "### scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cbbf8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing only score greater than 0.3\n",
      "\n",
      "Sentence:  La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona?\n",
      "neutral score is: 0.8939\n",
      "\n",
      "Sentence:  La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona\n",
      "positive score is: 0.9509\n",
      "\n",
      "Sentence:  La vostra organizzazione offre servizi pessimi\n",
      "negative score is: 0.9852\n",
      "\n",
      "Sentence:  La vostra organizzazione offre servizi non adeguati\n",
      "negative score is: 0.9593\n",
      "\n",
      "Sentence:  Sono molto soddisfatto del tuo lavoro\n",
      "positive score is: 0.9984\n",
      "\n",
      "Sentence:  non sono del tutto sicuro che il lavoro sia adeguato\n",
      "negative score is: 0.6756\n",
      "neutral score is: 0.3186\n",
      "\n",
      "Sentence:  l'azienda dovrebbe offrire servizi migliori\n",
      "positive score is: 0.9908\n",
      "\n",
      "Sentence:  il supporto offerto dal customer care non è stato adeguato\n",
      "negative score is: 0.9884\n",
      "\n",
      "Sentence:  il risultato è pessimo\n",
      "negative score is: 0.9974\n",
      "\n",
      "Sentence:  il Napoli ha giocato una partita ottima\n",
      "positive score is: 0.9865\n",
      "\n",
      "Sentence:  il lavoro dell'allenatore è stato modesto\n",
      "neutral score is: 0.9705\n",
      "\n",
      "\n",
      "CPU times: user 2.15 s, sys: 33.9 ms, total: 2.19 s\n",
      "Wall time: 546 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_sentences = [\n",
    "    \"La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona?\",\n",
    "    \"La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona\",\n",
    "    \"La vostra organizzazione offre servizi pessimi\",\n",
    "    \"La vostra organizzazione offre servizi non adeguati\",\n",
    "    \"Sono molto soddisfatto del tuo lavoro\",\n",
    "    \"non sono del tutto sicuro che il lavoro sia adeguato\",\n",
    "    \"l'azienda dovrebbe offrire servizi migliori\",\n",
    "    \"il supporto offerto dal customer care non è stato adeguato\",\n",
    "    \"il risultato è pessimo\",\n",
    "    \"il Napoli ha giocato una partita ottima\",\n",
    "    \"il lavoro dell'allenatore è stato modesto\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "labels_score = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "print(f\"Showing only score greater than {THR}\")\n",
    "print()\n",
    "\n",
    "for sentence in input_sentences:\n",
    "        \n",
    "        negative, neutral, positive = predict(sentence, tokenizer, model)\n",
    "        \n",
    "        neg_score = round(negative.item(), DEC_DIGITS)\n",
    "        pos_score = round(positive.item(), DEC_DIGITS)\n",
    "        neutr_score = round(neutral.item(), DEC_DIGITS)\n",
    "        \n",
    "        list_scores = [pos_score, neg_score, neutr_score]\n",
    "    \n",
    "        print(\"Sentence: \", sentence)\n",
    "    \n",
    "        for i, score in enumerate(list_scores):\n",
    "            if score > THR:\n",
    "                print(f\"{labels_score[i]} score is: {score}\")\n",
    "            \n",
    "            # formatting\n",
    "        print()\n",
    "            \n",
    "            \n",
    "# formatting\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24a4abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.8324,  0.2546,  3.3162],\n",
      "        [-3.5013, -0.8748,  4.5212]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "torch.Size([2, 3])\n",
      "CPU times: user 213 ms, sys: 4.03 ms, total: 217 ms\n",
      "Wall time: 53.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# batch scoring\n",
    "#\n",
    "\n",
    "input_sentences = [\n",
    "    \"un lavoro facile\",\n",
    "    \"un lavoro molto facile\"\n",
    "]\n",
    "\n",
    "proba_tensor = batch_predict(input_sentences, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b12e5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing only score greater than 0.2\n",
      "\n",
      "Sentence:  un lavoro facile\n",
      "positive score is: 0.9546\n",
      "\n",
      "Sentence:  un lavoro molto facile\n",
      "positive score is: 0.9952\n",
      "\n"
     ]
    }
   ],
   "source": [
    "THR = 0.2\n",
    "DEC_DIGITS = 4\n",
    "\n",
    "labels_score = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "print(f\"Showing only score greater than {THR}\")\n",
    "print()\n",
    "\n",
    "for i, sentence in enumerate(input_sentences):\n",
    "    negative, neutral, positive = proba_tensor[i]\n",
    "    \n",
    "    neg_score = round(negative.item(), DEC_DIGITS)\n",
    "    pos_score = round(positive.item(), DEC_DIGITS)\n",
    "    neutr_score = round(neutral.item(), DEC_DIGITS)\n",
    "        \n",
    "    list_scores = [pos_score, neg_score, neutr_score]\n",
    "    \n",
    "    print(\"Sentence: \", sentence)\n",
    "    \n",
    "    for i, score in enumerate(list_scores):\n",
    "        if score > THR:\n",
    "            print(f\"{labels_score[i]} score is: {score}\")\n",
    "            \n",
    "    # formatting\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f9afd",
   "metadata": {},
   "source": [
    "### Save in the model catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95f1af",
   "metadata": {},
   "source": [
    "### Test the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd64e895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing only score greater than 0.2\n",
      "\n",
      "Sentence:  La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona?\n",
      "neutral score is: 0.8939\n",
      "neutral score is: 0.8939\n",
      "\n",
      "Sentence:  La gestione da parte della Regione Lazio della complessa macchina dei vaccini è stata buona\n",
      "positive score is: 0.9509\n",
      "positive score is: 0.9396\n",
      "\n",
      "Sentence:  La vostra organizzazione offre servizi pessimi\n",
      "negative score is: 0.9852\n",
      "negative score is: 0.7595\n",
      "neutral score is: 0.2395\n",
      "\n",
      "Sentence:  La vostra organizzazione offre servizi non adeguati\n",
      "negative score is: 0.9593\n",
      "negative score is: 0.3279\n",
      "neutral score is: 0.6707\n",
      "\n",
      "Sentence:  Sono molto soddisfatto del tuo lavoro\n",
      "positive score is: 0.9984\n",
      "positive score is: 0.9909\n",
      "\n",
      "Sentence:  non sono del tutto sicuro che il lavoro sia adeguato\n",
      "negative score is: 0.6756\n",
      "neutral score is: 0.3186\n",
      "neutral score is: 0.8476\n",
      "\n",
      "Sentence:  l'azienda dovrebbe offrire servizi migliori\n",
      "positive score is: 0.9908\n",
      "neutral score is: 0.8356\n",
      "\n",
      "Sentence:  il supporto offerto dal customer care non è stato adeguato\n",
      "negative score is: 0.9884\n",
      "negative score is: 0.9444\n",
      "\n",
      "Sentence:  il risultato è pessimo\n",
      "negative score is: 0.9974\n",
      "negative score is: 0.9717\n",
      "\n",
      "Sentence:  il Napoli ha giocato una partita ottima\n",
      "positive score is: 0.9865\n",
      "positive score is: 0.7013\n",
      "neutral score is: 0.298\n",
      "\n",
      "Sentence:  il lavoro dell'allenatore è stato modesto\n",
      "neutral score is: 0.9705\n",
      "neutral score is: 0.9845\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "THR = 0.2\n",
    "DEC_DIGITS = 4\n",
    "\n",
    "labels_score = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "print(f\"Showing only score greater than {THR}\")\n",
    "print()\n",
    "\n",
    "for i, sentence in enumerate(input_sentences):\n",
    "        \n",
    "        negative, neutral, positive = predict(sentence, tokenizer, model)\n",
    "        negativeB, neutralB, positiveB = proba_tensor[i]\n",
    "        \n",
    "        neg_score = round(negative.item(), DEC_DIGITS)\n",
    "        pos_score = round(positive.item(), DEC_DIGITS)\n",
    "        neutr_score = round(neutral.item(), DEC_DIGITS)\n",
    "        \n",
    "        neg_scoreB = round(negativeB.item(), DEC_DIGITS)\n",
    "        pos_scoreB = round(positiveB.item(), DEC_DIGITS)\n",
    "        neutr_scoreB = round(neutralB.item(), DEC_DIGITS)\n",
    "        \n",
    "        list_scores = [pos_score, neg_score, neutr_score]\n",
    "        list_scoresB = [pos_scoreB, neg_scoreB, neutr_scoreB]\n",
    "        \n",
    "        print(\"Sentence: \", sentence)\n",
    "    \n",
    "        for i, score in enumerate(list_scores):\n",
    "            if score > THR:\n",
    "                print(f\"{labels_score[i]} score is: {score}\")\n",
    "        for i, scoreB in enumerate(list_scoresB):\n",
    "            if scoreB > THR:\n",
    "                print(f\"{labels_score[i]} score is: {scoreB}\")\n",
    "                \n",
    "            # formatting\n",
    "        print()\n",
    "            \n",
    "            \n",
    "# formatting\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b623e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_p37_cpu_v2]",
   "language": "python",
   "name": "conda-env-nlp_p37_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
