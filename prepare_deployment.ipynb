{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92efd8fd",
   "metadata": {},
   "source": [
    "### Sentiment Analysis: prepare deployment\n",
    "\n",
    "* use class definition in sentiment_analyzers file \n",
    "* Conda env used: Natural Language Processing for CPU Python 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b10102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# HuggingFace transformers (availale in OCI DS conda nlp env)\n",
    "# see: https://github.com/huggingface/transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# from my Python file\n",
    "from sentiment_analyzers import MultiSentimentAnalyzer\n",
    "\n",
    "import os\n",
    "from ads import set_auth\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from ads.common.model_metadata import (MetadataCustomCategory,\n",
    "                                       UseCaseType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45685199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deployment directory, if not exists\n",
    "PATH_ARTEFACT = f\"./nlptown-checkpoint\"\n",
    "\n",
    "if not os.path.exists(PATH_ARTEFACT):\n",
    "    os.mkdir(PATH_ARTEFACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a900a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy sentiment_analyzers.py to PATH_ARTEFACT\n",
    "!cp sentiment_analyzers.py $PATH_ARTEFACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baab3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizers\n",
    "# Load tokenizer and PyTorch weights form the Hub\n",
    "MODEL_NAME = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38049086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "LOCAL_DIR = PATH_ARTEFACT\n",
    "\n",
    "tokenizer.save_pretrained(LOCAL_DIR)\n",
    "model.save_pretrained(LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab166edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ADS:As force_overwrite is set to True, all the existing files in the ./nlptown-checkpoint will be removed\n",
      "WARNING:ads.common:Auto-extraction of taxonomy is not supported for the provided model. The supported models are lightgbm, xgboost, sklearn, keras, tensorflow, bert, transformers, torch, automl.\n"
     ]
    }
   ],
   "source": [
    "artifact = prepare_generic_model(model=model, model_path=PATH_ARTEFACT,\n",
    "                                 force_overwrite=True, \n",
    "                                 data_science_env=True,\n",
    "                                 use_case_type=UseCaseType.SENTIMENT_ANALYSIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7680ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./nlptown-checkpoint/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PATH_ARTEFACT}/score.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import io\n",
    "import logging \n",
    "\n",
    "# logging configuration - OPTIONAL \n",
    "logging.basicConfig(format='%(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger_pred = logging.getLogger('model-prediction')\n",
    "logger_pred.setLevel(logging.INFO)\n",
    "logger_feat = logging.getLogger('input sentence')\n",
    "logger_feat.setLevel(logging.INFO)\n",
    "\n",
    "# from my Python file\n",
    "from sentiment_analyzers import MultiSentimentAnalyzer\n",
    "\n",
    "MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "\n",
    "# the class\n",
    "sent_analyzer = None\n",
    "\n",
    "# to enable/disable detailed logging\n",
    "DEBUG = True\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    global sent_analyzer\n",
    "    \n",
    "    model_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    contents = os.listdir(model_dir)\n",
    "    \n",
    "    # Load the model from the model_dir using the appropriate loader\n",
    "    logger_pred.info(model_dir)\n",
    "    \n",
    "    if MODEL_FILE_NAME in contents:\n",
    "        sent_analyzer = MultiSentimentAnalyzer(\n",
    "            model_dir, labels=[\"1 star\", \"2 star\", \"3 star\", \"4 star\", \"5 star\"])\n",
    "        \n",
    "        logger_pred.info(\"Loaded the model and tokenizer !!!\")\n",
    "                \n",
    "    else:\n",
    "        raise Exception('{0} is not found in model directory {1}'.format(LOCAL_DIR, model_dir))\n",
    "    \n",
    "    return sent_analyzer\n",
    "\n",
    "def pre_inference(data):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Data format as expected by the predict API of the core estimator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: Data format after any processing.\n",
    "\n",
    "    \"\"\"\n",
    "    logger_pred.info(\"Preprocessing...\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def post_inference(yhat):\n",
    "    \"\"\"\n",
    "    Post-process the model results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yhat: Data format after calling model.predict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    yhat: Data format after any processing.\n",
    "\n",
    "    \"\"\"\n",
    "    logger_pred.info(\"Postprocessing output...\")\n",
    "    \n",
    "    return yhat\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Pandas DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: {'prediction': output from model.predict method}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if DEBUG:\n",
    "        logger_pred.info(\"In function predict...\")\n",
    "        \n",
    "    \n",
    "    if DEBUG:\n",
    "        logger_pred.info(\"Input data:\")\n",
    "        logger_pred.info(type(data))\n",
    "        logger_pred.info(data)\n",
    "    \n",
    "    # some check\n",
    "    assert sent_analyzer is not None, \"Model is not loaded\"\n",
    "    \n",
    "    scores = sent_analyzer.score(data)\n",
    "    \n",
    "    if DEBUG:\n",
    "        logger_pred.info(scores)\n",
    "    \n",
    "    # post inference not needed\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ead061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runtime.yaml', 'vocab.txt', 'config.json', 'special_tokens_map.json', 'sentiment_analyzers.py', 'score.py', 'test_json_output.json', '__pycache__', 'tokenizer_config.json', '.ipynb_checkpoints', 'pytorch_model.bin', 'tokenizer.json']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test key</th>\n",
       "      <th>Test name</th>\n",
       "      <th>Result</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runtime_env_path</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runtime_env_python</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runtime_path_exist</td>\n",
       "      <td>Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runtime_version</td>\n",
       "      <td>Check that field MODEL_ARTIFACT_VERSION is set to 3.0</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runtime_yaml</td>\n",
       "      <td>Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>score_load_model</td>\n",
       "      <td>Check that load_model() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>score_predict</td>\n",
       "      <td>Check that predict() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score_predict_arg</td>\n",
       "      <td>Check that all other arguments in predict() are optional and have default values</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>score_predict_data</td>\n",
       "      <td>Check that the only required argument for predict() is named \"data\"</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>score_py</td>\n",
       "      <td>Check that the file \"score.py\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>score_syntax</td>\n",
       "      <td>Check for Python syntax errors</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test key  \\\n",
       "0     runtime_env_path   \n",
       "1   runtime_env_python   \n",
       "2   runtime_path_exist   \n",
       "3      runtime_version   \n",
       "4         runtime_yaml   \n",
       "5     score_load_model   \n",
       "6        score_predict   \n",
       "7    score_predict_arg   \n",
       "8   score_predict_data   \n",
       "9             score_py   \n",
       "10        score_syntax   \n",
       "\n",
       "                                                                                                Test name  \\\n",
       "0                                             Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set   \n",
       "1           Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher   \n",
       "2                             Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.   \n",
       "3                                                   Check that field MODEL_ARTIFACT_VERSION is set to 3.0   \n",
       "4   Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory   \n",
       "5                                                                      Check that load_model() is defined   \n",
       "6                                                                         Check that predict() is defined   \n",
       "7                        Check that all other arguments in predict() are optional and have default values   \n",
       "8                                     Check that the only required argument for predict() is named \"data\"   \n",
       "9       Check that the file \"score.py\" exists and is in the top level directory of the artifact directory   \n",
       "10                                                                         Check for Python syntax errors   \n",
       "\n",
       "    Result Message  \n",
       "0   Passed          \n",
       "1   Passed          \n",
       "2   Passed          \n",
       "3   Passed          \n",
       "4   Passed          \n",
       "5   Passed          \n",
       "6   Passed          \n",
       "7   Passed          \n",
       "8   Passed          \n",
       "9   Passed          \n",
       "10  Passed          "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact.introspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd6630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact:/tmp/saved_model_99e0737d-8e61-4a74-a8c8-4709e99f451b.zip\n"
     ]
    }
   ],
   "source": [
    "# Saving the model artifact to the model catalog.\n",
    "compartment_id = os.environ['NB_SESSION_COMPARTMENT_OCID']\n",
    "project_id = os.environ['PROJECT_OCID']\n",
    "\n",
    "set_auth(auth='resource_principal')\n",
    "\n",
    "#\n",
    "# Save to Model Catalog\n",
    "#\n",
    "catalog_entry = artifact.save(display_name='ITA2 Sentiment analysis', \n",
    "                              description='A model for sentiment analysis',\n",
    "                              # to avoid to commit (be careful)\n",
    "                              ignore_pending_changes=True,\n",
    "                              # needs a longer timeout (the bin file is 600MB)\n",
    "                              timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87d1d4",
   "metadata": {},
   "source": [
    "### test the score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7194f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:model-prediction:/home/datascience/italian-sentiment-analysis/nlptown-checkpoint\n",
      "Loading model...\n",
      "Model loading completed!\n",
      "INFO:model-prediction:Loaded the model and tokenizer !!!\n"
     ]
    }
   ],
   "source": [
    "# %reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# add the path of score.py: \n",
    "\n",
    "import sys \n",
    "sys.path.insert(0, PATH_ARTEFACT)\n",
    "\n",
    "from score import load_model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "484589be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:model-prediction:/home/datascience/italian-sentiment-analysis/nlptown-checkpoint\n",
      "Loading model...\n",
      "Model loading completed!\n",
      "INFO:model-prediction:Loaded the model and tokenizer !!!\n"
     ]
    }
   ],
   "source": [
    "_ = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f4d7503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:model-prediction:In function predict...\n",
      "INFO:model-prediction:Input data:\n",
      "INFO:model-prediction:<class 'str'>\n",
      "INFO:model-prediction:Sono soddisfatto dei servizi offerti dalla vostra azienda\n",
      "INFO:model-prediction:[{'label': '1 star', 'score': 0.0029}, {'label': '2 star', 'score': 0.0041}, {'label': '3 star', 'score': 0.0824}, {'label': '4 star', 'score': 0.4626}, {'label': '5 star', 'score': 0.448}]\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(\"Sono soddisfatto dei servizi offerti dalla vostra azienda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b15d907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '1 star', 'score': 0.0029},\n",
       " {'label': '2 star', 'score': 0.0041},\n",
       " {'label': '3 star', 'score': 0.0824},\n",
       " {'label': '4 star', 'score': 0.4626},\n",
       " {'label': '5 star', 'score': 0.448}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef791806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_p37_cpu_v2]",
   "language": "python",
   "name": "conda-env-nlp_p37_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
